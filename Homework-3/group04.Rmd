---
title: "Homework - 3"
author: "Group 04"
output:
  pdf_document: default
  html_notebook: default
  word_document: default
---

```{r setup, include=FALSE, warning = FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
```

# Problem 1: Gradient Descent Algorithm for Multiple Linear Regression

The file concrete.csv includes 1,030 types of concrete with numerical features indicating
characteristics of the concrete. The variable “strength” is treated as the response variable.

• Standardize all variables (including the response variable “strength”). Split the data set
into a training set (60%) and a validation set (40%).

• Implement the gradient descent algorithm in R with the ordinary least square cost
function.

• Fit the multiple linear regression model using the gradient descent algorithm and the
training set. Try out different learning rates: alpha = 0.01,0.1,0.3,0.5 and compare the speed
of convergence by plotting the cost function. Determine the number of iterations needed
for each alpha value.

• Apply the fitted regression model to the validation set and evaluate the model
performance (ME, RMSE, MAE, MPE, MPAE). Calculate the correlation between the
predicted strength and the actual strength. Create a lift chart to show model
performance.

``` {r}
# Import Required Packages
library(dplyr)
library(data.table)
library(reshape2)
library(car)
library(readxl)
library(lubridate)
library(MLmetrics)
library(moments)
library(magrittr)
library(ggplot2)

# Read the csv file
df <- data.table(read.csv("concrete.csv"))

# Scale the dataframe
df <- as.data.frame(scale(df))

# Split into train and validation datasets
training_rows <- sample(seq_len(nrow(df)), size = floor(0.6 * nrow(df)))

train_data <- df[training_rows, ]
validation_data <- df[-training_rows, ]
```

Implementing Gradient Descent algorithm with the Ordinary Least Square cost function.

``` {r}
# Define the gradient descent function
gradient_desc <- function(x, y, lr, iters) {
  # First we create a list to keep the track
  # of the cost function for each iteration
  losses <- list()

  # Convert y to a matrix
  y <- as.matrix(y)

  # create a column of 1
  ones <- rep(1, dim(x)[[1]])
  # append it to the input (this is our X0)
  X <- as.matrix(cbind(ones, x))
  # Calculate number of samples
  n <- length(y)

  # Initialize model parameters/coefficients
  theta <- as.matrix(rnorm(n = dim(X)[2], 0, 1))

  # Calculate model predictions
  y_hat <- X %*% theta

  # calculate the loss using OLS cost function
  loss <- sum((y_hat - y)^2) / (2 * n)

  # Calculate the gradients of the cost function
  grads <- t(X) %*% (y_hat - y)

  # Update theta
  theta <- theta - lr * (1 / n) * grads


  # That was the first iteration of the gradient descent algorithm
  # Let's add the cost function to the list
  losses[[1]] <- loss


  counter <- 0
  # Number of iterations required to get the lowest loss
  sufficient_iterations <- 0
  for (i in 1:iters) {
    # Calculate model predictions
    y_hat <- X %*% theta

    # Calculate the loss using OLS cost function
    loss <- sum((y_hat - y)^2) / (2 * n)

    # Calculate the gradients
    grads <- t(X) %*% (y_hat - y)

    # Update theta
    theta <- theta - lr * (1 / n) * grads

    # Add cost to the list
    losses[[i + 1]] <- loss

    if (round(losses[[i]], 4) <= round(loss, 4)) {
      if (counter > 6) {
        break
      } else {
        counter <- counter + 1
        sufficient_iterations <- sufficient_iterations + 1
      }
    } else {
      counter <- 0
      sufficient_iterations <- sufficient_iterations + 1
    }
  }


  sufficient_iterations <- sufficient_iterations - counter
  # return the theta (aka model weights)
  return(list(
    "coeffs" = theta,
    "losses" = losses,
    "iterations_required" = sufficient_iterations,
    "final_loss" = loss
  ))
}

# Predict function
predict <- function(x, theta) {
  ones <- rep(1, dim(x)[[1]])
  # append it to the input (this is our X0)
  X <- as.matrix(cbind(ones, x))

  return(X %*% t(theta))
}
```

Now we create and train 4 models each with a different learning rate

``` {r warning = FALSE, message = FALSE}

# Model 1, lr = 0.01
model1 <- gradient_desc(train_data[, 1:8], train_data$strength, lr = 0.01, iters = 10000)

model1_weights <- t(model1$coeffs)
model1_losses <- melt(data.frame(model1$losses))
model1_losses$index <- 1:dim(model1_losses)[[1]]


# Model 2, lr = 0.10
model2 <- gradient_desc(train_data[, 1:8], train_data$strength, lr = 0.10, iters = 10000)

model2_weights <- t(model2$coeffs)
model2_losses <- melt(data.frame(model2$losses))
model2_losses$index <- 1:dim(model2_losses)[[1]]

# Model 3, lr = 0.30
model3 <- gradient_desc(train_data[, 1:8], train_data$strength, lr = 0.30, iters = 10000)

model3_weights <- t(model3$coeffs)
model3_losses <- melt(data.frame(model3$losses))
model3_losses$index <- 1:dim(model3_losses)[[1]]

# Model 4, lr = 0.50
model4 <- gradient_desc(train_data[, 1:8], train_data$strength, lr = 0.50, iters = 10000)

model4_weights <- t(model4$coeffs)
model4_losses <- melt(data.frame(model4$losses))
model4_losses$index <- 1:dim(model4_losses)[[1]]
```

Let's plot the loss vs number of iterations for each model to evaluate their performance.

``` {r}
# Model 1
ggplot(model1_losses, aes(x = index, y = value)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = model1$iterations_required) +
  geom_text(x = model1$iterations_required / 2, y = 3.5, label = "Final Loss") +
  geom_text(x = model1$iterations_required / 2, y = 3, label = as.character(round(model1$final_loss, 4))) +
  geom_text(
    x = model1$iterations_required,
    y = 1,
    label = as.character(model1$iterations_required),
    check_overlap = TRUE
  ) +
  geom_text(
    x = model1$iterations_required,
    y = 1.25,
    label = "Iterations Needed",
    check_overlap = TRUE
  ) +
  geom_line(color = "red") +
  labs(x = "Iteration", y = "Loss") +
  ggtitle("Model 1 Performance (Learning Rate = 0.01)")

# Model 2
ggplot(model2_losses, aes(x = index, y = value)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = model2$iterations_required) +
  geom_text(x = model2$iterations_required / 2, y = 3.5, label = "Final Loss") +
  geom_text(x = model2$iterations_required / 2, y = 3, label = as.character(round(model2$final_loss, 4))) +
  geom_text(
    x = model2$iterations_required,
    y = 1,
    label = as.character(model2$iterations_required),
    check_overlap = TRUE
  ) +
  geom_text(
    x = model2$iterations_required,
    y = 1.25,
    label = "Iterations Needed",
    check_overlap = TRUE
  ) +
  geom_line(color = "red") +
  labs(x = "Iteration", y = "Loss") +
  ggtitle("Model 2 Performance (Learning Rate = 0.10)")

# Model 3
ggplot(model3_losses, aes(x = index, y = value)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = model3$iterations_required) +
  geom_text(x = model3$iterations_required / 2, y = 3.5, label = "Final Loss") +
  geom_text(x = model3$iterations_required / 2, y = 3, label = as.character(round(model3$final_loss, 4))) +
  geom_text(
    x = model3$iterations_required,
    y = 1,
    label = as.character(model3$iterations_required),
    check_overlap = TRUE
  ) +
  geom_text(
    x = model3$iterations_required,
    y = 1.25,
    label = "Iterations Needed",
    check_overlap = TRUE
  ) +
  geom_line(color = "red") +
  labs(x = "Iteration", y = "Loss") +
  ggtitle("Model 3 Performance (Learning Rate = 0.30)")

# Model 4
ggplot(model4_losses, aes(x = index, y = value)) +
  geom_point(alpha = 0.5) +
  geom_vline(xintercept = model4$iterations_required) +
  geom_text(x = model4$iterations_required / 2, y = 3.5, label = "Final Loss") +
  geom_text(x = model4$iterations_required / 2, y = 3, label = as.character(round(model4$final_loss, 4))) +
  geom_text(
    x = model4$iterations_required,
    y = 1,
    label = as.character(model4$iterations_required),
    check_overlap = TRUE
  ) +
  geom_text(
    x = model4$iterations_required,
    y = 1.25,
    label = "Iterations Needed",
    check_overlap = TRUE
  ) +
  geom_line(color = "red") +
  labs(x = "Iteration", y = "Loss") +
  ggtitle("Model 4 Performance (Learning Rate = 0.50)")
```

``` {r}
cat("Number of iterations required for each model are :\n")
cat("Model 1:", as.character(model1$iterations_required), "\n")
cat("Model 2:", as.character(model2$iterations_required), "\n")
cat("Model 3:", as.character(model3$iterations_required), "\n")
cat("Model 4:", as.character(model4$iterations_required), "\n")
```

As observed, the model converges faster as the learning rate increases.

Testing the model on the validation data and calulcating errors -

``` {r}
# We define the Mean Error function
ME <- function(y_hat, y) {
  sum(y - y_hat) / length(y)
}

# We define the Mean Percentage Error Function
MPE <- function(y_hat, y) {
  (sum((y - y_hat) / y)) / length(y)
}
```


Now let's look at the model statstics - 
``` {r}
model1_predictions <- predict(validation_data[, 1:8], model1_weights)

cat("----Model 1 Summary ----\n")
cat("MAE:", MAE(model1_predictions, validation_data[, 9]), "\n")
cat("RMSE:", RMSE(model1_predictions, validation_data[, 9]), "\n")
cat("ME:", ME(model1_predictions, validation_data[, 9]), "\n")
cat("MPE:", MPE(model1_predictions, validation_data[, 9]), "\n")
cat("MPAE", MAPE(model1_predictions, validation_data[, 9]), "\n")

model2_predictions <- predict(validation_data[, 1:8], model2_weights)

cat("----Model 2 Summary ---- \n")
cat("MAE:", MAE(model2_predictions, validation_data[, 9]), "\n")
cat("RMSE:", RMSE(model2_predictions, validation_data[, 9]), "\n")
cat("ME:", ME(model2_predictions, validation_data[, 9]), "\n")
cat("MPE:", MPE(model2_predictions, validation_data[, 9]), "\n")
cat("MPAE", MAPE(model2_predictions, validation_data[, 9]), "\n")

model3_predictions <- predict(validation_data[, 1:8], model3_weights)

cat("----Model 3 Summary ----\n")
cat("MAE:", MAE(model3_predictions, validation_data[, 9]), "\n")
cat("RMSE:", RMSE(model3_predictions, validation_data[, 9]), "\n")
cat("ME:", ME(model3_predictions, validation_data[, 9]), "\n")
cat("MPE:", MPE(model3_predictions, validation_data[, 9]), "\n")
cat("MPAE", MAPE(model3_predictions, validation_data[, 9]), "\n")

model4_predictions <- predict(validation_data[, 1:8], model4_weights)

cat("----Model 4 Summary ----\n")
cat("MAE:", MAE(model4_predictions, validation_data[, 9]), "\n")
cat("RMSE:", RMSE(model4_predictions, validation_data[, 9]), "\n")
cat("ME:", ME(model4_predictions, validation_data[, 9]), "\n")
cat("MPE:", MPE(model4_predictions, validation_data[, 9]), "\n")
cat("MPAE", MAPE(model4_predictions, validation_data[, 9]), "\n")
```

We can see that all the models have approximately the same accuracy regardless the learning rate.

Calculating the correlation between predicted strength and actual strength

``` {r}

cat("The correlation is :", cor(model1_predictions, validation_data[, 9]), "\n")
```

Plotting a lift chart 

``` {r}
# Create a temp data frame to calculate the sumulative strength
temp <- data.frame("strength" = order(validation_data[, 9]))
temp$cumstrength <- cumsum(temp$strength)
temp$samples <- 1:dim(temp)[[1]]

# Plot the lift chart
ggplot(temp, aes(x = samples, y = cumstrength)) +
  geom_line() +
  labs(x = "Number of Samples", y = "Total Strength") +
  ggtitle("Lift Chart")

# Delete all environment variables
rm(list = ls())
```

---

# Problem 2

• Read the included research article “Modeling Slump Flow Concrete”. It is sufficient to
consider “Slump Flow” as the response variable in this problem just as in the included
article.

• Create a scatterplot matrix of “Concrete Slump Test Data” and select an initial set of
predictor variables.

• Build a few potential regression models using “Concrete Slump Test Data”

• Perform regression diagnostics using both typical approach and enhanced approach

• Identify unusual observations and take corrective measures

• Select the best regression model

• Fine tune the selection of predictor variables

• Interpret the prediction results


## In our opinion, it is sufficient to consider "Slump Flow" as the response variable because the slump flow is a function of the content of all concrete ingredients including cement, fly ash, blast furnace slag, water, superplasticizer, coarse aggregate, and fine aggregate. And since HPC is already so complicated to model, incorporating another response variable may overcomplicate the model hypothesis.


``` {r}
df <- readxl::read_xlsx("Concrete Slump Test Data.xlsx", sheet = "Concrete slump")
df <- df[, 2:11]

# Let's plot the scatterplot matrix
scatterplotMatrix(df, main = "Scatterplot Matrix")


# Since the above matrix is hard to interpret, we only plot it for a select
# variables
scatterplotMatrix(~ Cement + Slag + Water + SP + `Slump Flow`,
  data = df,
  main = "Scatterplot Matrix"
)
```

Let's build a few regression models using these predictor variables 

``` {r}
fit1 <- lm(`Slump Flow` ~ Water + `Coarse Aggregate` + `Fine Aggregate`, data = df)

summary(fit1)
```

``` {r}
fit2 <- lm(`Slump Flow` ~ Water + Slag + `Fine Aggregate`, data = df)

summary(fit2)
```

Let's try and fit a quadratic model

``` {r}
fit3 <- lm(`Slump Flow` ~ (Water^2) + Water + Slag, data = df)

summary(fit3)
```
## Performing Regression Diagnostics using Typical Approach

``` {r}

# Model 1
par(mfrow = c(2, 2))
plot(fit1)

# Model 2
par(mfrow = c(2, 2))
plot(fit2)

# Model 3
par(mfrow = c(2, 2))
plot(fit3)
```

Model 3 seems to be the best fit. We can also see that points 41 and 69 appear to be influential. We can remove these two points from the data to see if the model fits better.

``` {r}
fit3 <- lm(`Slump Flow` ~ (Water^2) + Water + Slag, data = df[-c(41, 69), ])

# Model 3
par(mfrow = c(2, 2))
plot(fit3)
```

The model seems to fit the data quite well. 

## Peforming Diagnostic Regression with Enhanced Approach

### Normality

``` {r}
fit <- lm(`Slump Flow` ~ Water + `Coarse Aggregate` + `Fine Aggregate`, data = df)

qqPlot(fit, labels = rownames(df), id.method = "identify", simulate = TRUE, main = "QQ Plot")
```

We can see from the QQ Plot that our model satisfies normality. Almost all the points fall on the 45 degree line except for a few.


### Independence of Errors

``` {r}
durbinWatsonTest(fit)
```

Since the p-value is insignificant, there is no autocorrelation and hence and independence of errors.


### Linearity

``` {r}
crPlots(fit)
```

It seems that this model satisfies linearity.

## Homoscedasticity

``` {r}
ncvTest(fit)
spreadLevelPlot(fit)
```

From the insignificant p-value and the Spread-Level Plot we can see that the model meets the requirements for Homoscedasticity.

## Unusual Observations and Corrective Measures

``` {r}
outlierTest(fit)
```
We can see that point 69 is an outlier. But the p-value is not significant and hence we can leave the model as it is.

Let's search for High Leverage points

``` {r}
hat.plot <- function(fit) {
  p <- length(coefficients(fit))
  n <- length(fitted(fit))
  plot(hatvalues(fit),
    main = "Index Plot of Hat Values"
  )
  abline(h = c(2, 3) * p / n, col = "red", lty = 2)
  identify(1:n, hatvalues(fit), names(hatvalues(fit)))
}

hat.plot(fit)
```

We can see that points 8, 12, 14, and 78 are unusual when it comes to their predicted values.

## Influential Observations

``` {r}
cutoff <- 4 / (nrow(df) - length(fit$coefficients) - 2)
plot(fit, which = 4, cook.levels = cutoff)
abline(h = cutoff, lty = 2, col = "red")
```


``` {r}
influencePlot(fit,
  main = "Influence Plot",
  sub = "Circle Size is proportional to Cook's distance"
)
```

The plot shows that 41 and 14 are outliers. 8 and 14 have high leverage.  45, 41, 8 and 14 are influential observations.

We remove points 41 and 14 as they are outliers as well as influential.

``` {r}
fit <- lm(`Slump Flow` ~ Water + `Coarse Aggregate` + `Fine Aggregate`, data = df[-c(14, 41), ])
fit2 <- lm(`Slump Flow` ~ Water + Slag + `Coarse Aggregate` + `Fine Aggregate`, data = df[-c(14, 41), ])
fit3 <- lm(`Slump Flow` ~ (Water^2) + Water + Slag, data = df[-c(14, 41), ])
```

## Selecting the best regression model

``` {r}
anova(fit2, fit)
anova(fit2, fit3)

AIC(fit, fit2, fit3)
```

The p-value test tells us that fit2 is better than fit since as the Slag predictor adds extra value to our model. However it is not better than fit3 model. 

The AIC test also indicated that fit3 is the best model.

## Let's interpret the results

``` {r}
summary(fit3)

predictions <- predict(fit3, df)

head(predictions)

rm(list = ls())
```


We can infer from the model coefficients Water is the most important predictor in calculating the value of the Slump Flow. 1 kg per M cube change in Water results to 0.57 cm change in the Slump Flow. Slag is a less important predictor.


---

# Problem 3


Importing Insurance dataset

```{r}
insurance <- read.csv("insurance.csv", stringsAsFactors = TRUE)

ins <- read.csv("insurance.csv", stringsAsFactors = TRUE)
```

Summary Statistics

```{r }
mean(insurance$charges)

median(insurance$charges)

min(insurance$charges)

max(insurance$charges)

quantile(insurance$charges, 0.25)

quantile(insurance$charges, 0.75)

skewness(insurance$charges)

kurtosis(insurance$charges)

ggplot(insurance, aes(x = charges)) +
  geom_histogram(binwidth = 1000)
```

Interpretation- The summary statistics, namely the mean and median indicate skewness
in the dependent variable and the skewness of 1.51418 tells us that it is highly skewed
the kurtosis value of 4.6 tells us that the data has a heavier tail than the normal distribution.
the histogram reinforces the above my showing a left skewed distribution with a heavy right tail.

## Let's plot the scatterplot matrix

```{r warning = FALSE}

attach(insurance)

x <- cbind(age, BMI, children, charges)

cor(x)

scatterplotMatrix(x, spread = FALSE, col = "blue", main = "ScatterPlot Matrix")


detach(insurance)
```

Interpretation- The scatter plot matrix shows a clear correlation between age-BMI, age-charges and BMI-charges. 

The values in correlation  are indicative of the same.

## Building Regression model

```{r }

fit1 <- lm(charges ~ ., data = insurance)

summary(fit1)

6062 / mean(insurance$charges)
```


Evaluation
- The RSE = 6062, meaning that the observed medical charges deviate from the predicted values 
by approximately 6062 units in average. 

- This corresponds to an error rate of 6062/mean(insurance$charges)= 45%

- The adjusted R squared value of 0.7494 indicates that a large proportion of the variability in 
the outcome has been explained by the regression model.

- A large F-statistic of 500.8 producing a p-value (p < 0.05) of 2.2e-16, is highly significant.



## Regression Diagnostics

## Typical approach

```{r }

par(mfrow = c(2, 2))
plot(fit1)
```


- Normality- The normal Q-Q plot shows that the normality assumption of the dependent variable 
has been violated since the points dont fall on the 45 degree line

- Linearity- The residuals versus fitted graph shows the presence of a
curved relationship between the residuals and the fitted values.

- Homoscedasticity-The Scale-location plot shows if residuals are spread equally along the ranges 
of predictors but here that does not seem to be the case as the spread does not look uniform accross 
all the predictors. The model doesnt seem to have meet this assumption.

- Outliers- The residuals vs leverage plot shows that there is no influential case, or cases. 
we can barely see Cook’s distance lines because all cases are well inside of the Cook’s distance lines.

## Enhanced Approach

## Normality

```{r }
qqPlot(fit1, labels = row.names(insurance), id.method = "identify", simulate = TRUE, main = "Q-Q Plot")
```


most of the points dont fall close to the line and inside the confidence interval suggesting that
the normality assumption has not been met

## Linearity

```{r}
crPlots(fit1)
```

the component plus residual plots show that age, BMI and children meet the linearity assumption.

## Homoscedasticity

```{r }
ncvTest(fit1)

spreadLevelPlot(fit1)
```

the suggested power transformation and the non horizontal line in spread level plot indicate that the 

Homoscedasticity assumption has been violated



grouping BMI(adding an indicator for obesity- 1, normal- 0)
adding a nonlinear term(quadratic) for age and performing MLR

```{r }
ins$BMI <- findInterval(ins$BMI, c(0, 30))

ins$BMI <- as.factor(ins$BMI)

levels(ins$BMI) <- c(0, 1)


fit2 <- lm(charges ~ age + I(age^2) + sex + BMI + children + smoker + region, data = ins)

summary(fit2)

rm(list = ls())
```

Comparison to the previous model

Evaluation

- The RSE = 6000, meaning that the observed medical charges deviate from the predicted values 
by approximately 6000 units in average which is 62 units less than the previous model 


- The adjusted R squared value of 0.7545 is greater that the previous model(0.7494) indicating that more 

- Variability in the outcome has been explained by the model than the previous 

---

# Problem 4 - . Multiple Linear Regression Model for Forest Fire Data

## Importing the dataset

``` {r}
Forest_Fires <- read_excel("Forest Fires Data.xlsx", sheet = "forestfires")
```

## Converting month and day to numerical variables

``` {r warning = TRUE}

Forest_Fires$month <- recode(Forest_Fires$Month,
  "jan" = 1, "feb" = 2, "mar" = 3,
  "apr" = 4, "may" = 5, "jun" = 6, "jul" = 7, "aug" = 8, "sep" = 9,
  "oct" = 10, "nov" = 11, "dec" = 12
)

Forest_Fires$day <- recode(Forest_Fires$Day,
  "sun" = 1, "mon" = 2, "tue" = 3,
  "wed" = 4, "thu" = 5, "fri" = 6, "sat" = 7
)



Forest_Fires <- Forest_Fires[, -c(3, 4)]
```

Plot the scatterplot matrix

```{r warning = FALSE}

scatterplotMatrix(Forest_Fires, main = "Scatterplot Matrix")
```

Since the above matrix is hard to interpret, we only plot it for a select
variables


```{r}
scatterplotMatrix(~ Wind + Temp + DMC + X + Y + month + day + `Area`,
  data = Forest_Fires,
  main = "Scatterplot Matrix"
)
```

building a few regression models

```{r}
mod1 <- lm(Area ~ Wind + Temp + DMC + X + Y + month + day, data = Forest_Fires)

summary(mod1)

mod2 <- lm(Area ~ Wind + Temp + X + day, data = Forest_Fires)

summary(mod2)

mod3 <- lm(Area ~ Wind + I(Temp^2) + X + day, data = Forest_Fires)

summary(mod3)

mod4 <- lm(Area ~ log(Wind) + I(Temp^2) + X + day, data = Forest_Fires)

summary(mod4)
```

## Performing Regression Diagnostics using Typical Approach

```{r}

# Model 1
par(mfrow = c(2, 2))
plot(mod1)


# Model 2
par(mfrow = c(2, 2))
plot(mod2)


# Model 3
par(mfrow = c(2, 2))
plot(mod3)


# Model 4
par(mfrow = c(2, 2))
plot(mod4)
```

Model 4 seems to be the best fit though all the models including model 5 seem to satisfy the
assumptions of Normality, Linearity and Homoscedasticity as depicted in the 
Normal Q_Q, Residuals vs Fitted ans Scale-Location plots Respectively.
We can also see that points 237 and 413 appear to be influential/outliers as shown in the Residuals vs Leverage. 
We can remove these two points from the data to see if the model fits better.

```{r}
mod4_new <- lm(Area ~ log(Wind) + I(Temp^2) + X + day, data = Forest_Fires[-c(416, 239), ])

# new Model 4
par(mfrow = c(2, 2))
plot(mod4_new)
```

## Peforming Diagnostic Regression with Enhanced Approach

### Normality

``` {r}

qqPlot(mod4_new, labels = rownames(Forest_Fires), id.method = "identify", simulate = TRUE, main = "QQ Plot")
```

#We can see from the QQ Plot that our model satisfies normality. 
#Almost all the points fall on the 45 degree line except for a few and most of them fall within the confidence Interval.



## Linearity

``` {r}
crPlots(mod4_new)
```

Apparently this model satisfies linearity.

### Homoscedasticity

``` {r}
ncvTest(mod4_new)
spreadLevelPlot(mod4_new)
```

From the insignificant p-value and the Spread-Level Plot we can see that the model meets the requirements for Homoscedasticity.


### Unusual Observations and Corrective Measures

``` {r}
outlierTest(mod4_new)
```

We can see that the point 478,238,237,236,419,377,235 are outliers.
As the p-value is not significant and we can leave the model as it is.

## High Leverage points

``` {r}
hat.plot <- function(mod4_new) {
  p <- length(coefficients(mod4_new))
  n <- length(fitted(mod4_new))
  plot(hatvalues(mod4_new),
    main = "Index Plot of Hat Values"
  )
  abline(h = c(2, 3) * p / n, col = "red", lty = 2)
  identify(1:n, hatvalues(mod4_new), names(hatvalues(mod4_new)))
}
hat.plot(mod4_new)
```

We can see that points 249, 291, 464 and 483 are unusual when it comes to their predicted values.


### Influential Observations

``` {r}
cutoff <- 4 / (nrow(df) - length(mod4_new$coefficients) - 2)
plot(mod4_new, which = 4, cook.levels = cutoff)
abline(h = cutoff, lty = 2, col = "red")
```


the Cook's Distance graph shows that 237,236,478 are influential.

``` {r}
influencePlot(mod4_new,
  main = "Influence Plot",
  sub = "Circle Size is proportional to Cook's distance"
)
```

The Influence plot shows that 237,238 and 478 are outliers and also highly influential observations.

We remove points 237,238 and 478 as they are outliers as well as influential.

``` {r}

mod1 <- lm(Area ~ Wind + Temp + DMC + X + Y + month + day, data = Forest_Fires[-c(237, 238, 478), ])


mod2 <- lm(Area ~ Wind + Temp + X + day, data = Forest_Fires[-c(237, 238, 478), ])


mod3 <- lm(Area ~ Wind + I(Temp^2) + X + day, data = Forest_Fires[-c(237, 238, 478), ])


mod4 <- lm(Area ~ log(Wind) + I(Temp^2) + X + day, data = Forest_Fires[-c(237, 238, 478), ])


mod5 <- lm(Area ~ I(Temp^2) + I(X^1.3) + I(day^10), data = Forest_Fires[-c(237, 238, 478), ])
```

## Selecting the best regression model

``` {r}
anova(mod2, mod1)

AIC(mod1, mod2, mod3, mod4)
```

As only model 1 and 2 are nested models the anova funcion is used on them.
The insignificant P value indicated that the excess predicitors in  the pairs 
dont add to the linear predictions so we are better off dropping them
i.e model 2 is the best among the pair

The AIC test indicated that model 4 is the best among them all.

Let's interpret the results

``` {r}
summary(mod4)
prediction <- predict(mod4, Forest_Fires)
head(prediction)

rm(list = ls())
```


Interpretation

The R squared and the adjusted R squared values are very low for all the linear regression models indicating its not the best fit for this data set. It can also be seen that the predicter "Temperature" plays a major role in predicting the area of forest fires 
RSE- 62.15, meaning that the predicted Area deviates from the actual values 
by approximately 62 units on average 
---
