---
title: "Homework-4"
author: "Group 4"
date: "6/4/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

``` {r}
# Import the required packages
library(dplyr)
library(readxl)
library(e1071)
library(FNN)
library(class)
library(modelr)
library(caret)
```

# Problem 1: Personal Loan Acceptance

The file UniversalBank.xlsx contains data on 5000 customers. The data include
customer demographic information (age, income, etc.), the customer's relationship with
the bank (mortgage, securities account, etc.), and the customer response to the last
personal loan campaign (Personal Loan). Among these 5000 customers, only 480(= 9.6%)
accepted the personal loan that was offered to them in the earlier campaign.
Partition the data into training (60%) and validation (40%) sets.

Importing the dataset

```{r}
universal <- read_xlsx("UniversalBank.xlsx", sheet = "Data")

test.df <- data.frame(
  Education1 = 0, Education2 = 1, Education3 = 0, Age = 40, Experience = 10, Income = 84, Family = 2,
  CCAvg = 2, Mortgage = 0, Personal.Loan = "", `Securities Account` = 0,
  `CD Account` = 0, Online = 1, Creditcard = 1
)
```


Creating Dummy variables for Education type

```{r}
attach(universal)

X <- universal[, 8]


X$Education <- paste0("Education", X$Education)
mm <- model.matrix(~ . + 0, data = X)
colnames(mm) <- c("Education1", "Education2", "Education3")

universal <- cbind(mm, universal[, -8])

detach(universal)
```

Normalizing

```{r}

train.index <- sample(rownames(universal), 0.6 * dim(universal)[1])
valid.index <- setdiff(rownames(universal), train.index)

train.df <- universal[train.index, -c(4, 8)]
valid.df <- universal[valid.index, -c(4, 8)]

train.norm.df <- train.df
valid.norm.df <- valid.df
universal.norm.df <- universal[, -c(4, 8)]

norm.values <- preProcess(train.df[, -10], method = c("center", "scale"))
train.norm.df <- predict(norm.values, train.df)
valid.norm.df <- predict(norm.values, valid.df)
universal.norm.df <- predict(norm.values, universal[, -c(4, 8)])

colnames(test.df) <- colnames(universal[, -c(4, 8)])

test.norm.df <- predict(norm.values, test.df)
```

Performing Knn

```{r}
KN <- knn(train = train.norm.df[, -10], test = test.norm.df[, -10], cl = train.norm.df[, 10], k = 1)
row.names(train.df)[attr(KN, "KN.index")]

KN
```

Interpretation- As apparent in the result this customer is classified as class 0(non loan acceptence)
for a default cutoff value of 0.5.


## (b) - predicting the choice of k that balances between overfitting and ignoring the predictor information

Computing Knn for different k values on validation set

```{r}
accuracy.df <- data.frame(k = seq(1, 20), accuracy = rep(0, 20))

valid.norm.df[, 10] <- as.factor(valid.norm.df[, 10])


for (i in 1:20) {
  knn.pred <- knn(train.norm.df[, -10], valid.norm.df[, -10], cl = train.norm.df[, 10], k = i)
  accuracy.df[i, 2] <- confusionMatrix(knn.pred, valid.norm.df[, 10])$overall[1]
}

max(accuracy.df[, 2])
```

as we can see the maximun accuracy(0.9655) is achieved with a k value of 3

## (c)

```{r}
KN1 <- knn(train.norm.df[, -10], valid.norm.df[, -10], cl = train.norm.df[, 10], k = 3)

valid.predict <- confusionMatrix(KN1, valid.norm.df[, 10])

valid.predict
```

## (d) the given customer is same as the one in question 1 classifing the customer using k=3

```{r}
KN2 <- knn(train = train.norm.df[, -10], test = test.norm.df[, -10], cl = train.norm.df[, 10], k = 3)
row.names(train.df)[attr(KN, "KN.index")]

KN2
```
As apparent in the result this customer is classified as class 0(non loan acceptence)


## (e)
Repartitioning the data into training, validation, and test sets (50% : 30% :20%)

```{r}
train.index1 <- sample(rownames(universal), 0.5 * dim(universal)[1])
index1 <- setdiff(rownames(universal), train.index1)
valid.index1 <- sample(rownames(universal[index1, ]), 0.6 * dim(universal[index1, ])[1])
test.index1 <- setdiff(rownames(universal[index1, ]), valid.index1)

train.df1 <- universal[train.index1, -c(4, 8)]
valid.df1 <- universal[valid.index1, -c(4, 8)]
test.df1 <- universal[test.index1, -c(4, 8)]
```

normalizing the data 

```{r}
train.norm.df1 <- train.df1
valid.norm.df1 <- valid.df1
test.norm.df1 <- test.df1

train.norm.df1 <- predict(norm.values, train.df1)
valid.norm.df1 <- predict(norm.values, valid.df1)
test.norm.df1 <- predict(norm.values, test.df1)

test.norm.df1[, 10] <- as.factor(test.norm.df1[, 10])
```

performing Knn for test set with training and validation sets

```{r}
KN3 <- knn(train.norm.df1[, -10], test.norm.df1[, -10], cl = train.norm.df1[, 10], k = 3)

KN4 <- knn(valid.norm.df1[, -10], test.norm.df1[, -10], cl = valid.norm.df1[, 10], k = 3)


test.predict1 <- confusionMatrix(KN3, test.norm.df1[, 10])

test.predict2 <- confusionMatrix(KN4, test.norm.df1[, 10])

test.predict1

test.predict2
```

interpretation- As it can be seen better accuracy in prediction(1-0.966,2-0.956)  was achieved when the training data set was larger.
in the 1st case 2500 objects were used as training data whereas in the 2nd part 1500 were used.
Hence it is safe to say that the prediction gets better with the best value of K and larger training sample.



# Problem 2: Predicting Housing Median Prices k-NN

The file BostonHousing.xlsx contains information on over 500 census tracts in
Boston, where for each tract 14 variables are recorded. The last column (CAT.MEDV)
was derived from MEDV, such that it obtains the value 1 if MEDV > 30 and 0 otherwise.
Consider the goal of predicting the median value (MEDV) of a tract, given the
information in the first 13 columns. Partition the data into training (60%) and validation
(40%) sets.


## (a) Perform a k-NN prediction with all 13 predictors (ignore the CAT.MEDV column), trying values of k from 1 to 5. Make sure to normalize the data (click "normalize input data"). What is the best k chosen? What does it mean?
``` {r warning = FALSE, message = FALSE}

# Import the BostonHousing.xlsx file
BostonHousing <- read_xlsx("BostonHousing.xlsx", sheet = "Data")

# Define the normalize function
normalize <- function(x) {
  return((x - min(x))) / (max(x) - min(x))
}

# Normalize the dataframe
df.norm <- as.data.frame(lapply(BostonHousing[1:13], normalize))

# Generate the training data indices
indices <- sample(seq_len(nrow(df.norm)), size = floor(0.6 * nrow(df.norm)))

# Get training and validation data
train_data <- df.norm[indices, ]
validation_data <- df.norm[-indices, ]

# Create a dataframe to keep track of k vs error
error.df <- data.frame("k" = 1:5, "error" = rep(0, 5))

# Loop for K = 1 to 5
for (i in 1:5) {
  model <- knnreg(x = train_data[, 1:12], y = train_data[, 13], k = i)
  error.df[i, 2] <- RMSE(validation_data[, 13], predict(model, validation_data[, 1:12]))
}

# Get the K-value with the lowest RMSE error
best_k <- filter(error.df, error == min(error.df$error))$k
cat("The model with the best K is:", best_k, "\n")
```

## (b) Predict the MEDV for a tract with the following information, using the best k:

``` {r}

# Let's get the model with the best K
model <- knnreg(x = train_data[, 1:12], y = train_data[, 13], k = best_k)

# Create a dataframe for the given record
df <- data.frame(
  "CRIM" = 0.2, "ZN" = 0, "INDUS" = 7,
  "CHAS" = 0, "NOX" = 0.538, "RM" = 6,
  "AGE" = 62, "DIS" = 4.7, "RAD" = 4,
  "TAX" = 307, "PTRATIO" = 21, "LSTAT" = 10
)

# Normalize the dataframe
df.norm <- as.data.frame(lapply(df[1:12], normalize))

# Predict the MEDV value for the new record.
prediction <- predict(model, df.norm)
cat("The MEDV prediction for the above record is:", prediction, "\n")
```

## (c) Why is the error of the training data zero?

The error for training data will be zero at K = 1, since the single closest neighbor to the training sample vector will be itself. Hence the error will be zero.

``` {r}
# Train the model with k = 1
model <- knnreg(x = train_data[, 1:12], y = train_data[, 13], k = 1)

# print
cat(
  "Error for Training Data at k = 1:",
  RMSE(train_data[, 13], predict(model, train_data[, 1:12])),
  "\n"
)

# remove all env variables
rm(list = ls())
```

## (d) Why is the validation data error overly optimistic compared to the error rate when applying this k-NN predictor to new data?

We have chosen a model which can can perform best on the validation data. So it makes total sense that the error of the model on validation data is more optimistic as compared to it's error on new data.

## (e) If the purpose is to predict MEDV for several thousands of new tracts, what would be the disadvantage of using k-NN prediction? List the operations that the algorithm goes through in order to produce each prediction.

- The disadvantage of using K-NN prediction for several thousand new tracts would be that the KNN algorithm will be slow. Calculating the euclidean distances for several thousand vectors would take a long time. Also, more sophisticated methods like Regression Trees and Neural Networks would perform better on such a large dataset.

**The algorithm goes through the following operations in order to produce each operation** -

For each record in the dataset, the algorithm - 

1. Calculates the euclidean distance of that vector(record) with every other record in the dataset.

2. Sorts the euclidean distances from the lowest to the highest.

3. Takes the top K neighbors and then -
   - If it is a classification problem, it takes the classes of each of the K neighbors and assigns the majority class to the current record.
   - If it is a regression problem, it takes the average of the output variable of each of the K neighbors and assigns it to the current record.
   
---

# Problem 3

The file Accidents.xlsx contains information on 42,183 actual automobile accidents
in 2001 in the United States that involved one of three levels of injury: NO INJURY,
INJURY, or FATALITY. For each accident, additional information is recorded, such as
day of week, weather conditions, and road type. A firm might be interested in 
developing a system for quickly classifying the severity of an accident based on initial
reports and associated data in the system (some of which rely on GPS-assisted
reporting).
Our goal here is to predict whether an accident just reported will involve an injury
(MAX_SEV_IR = 1 or 2) or will not (MAX_SEV_IR = 0). For this purpose, create a
dummy variable called INJURY that takes the value "yes" if MAX_SEV_IR = 1 or 2, and
otherwise "no."


## (a) Using the information in this dataset, if an accident has just been reported and no
further information is available, what should the prediction be? (INJURY = Yes or No?) Why?

``` {r}
# Import Accidents.xlsx
Accidents <- read_xlsx("Accidents.xlsx", sheet = "Data")

Accidents$INJURY <- Accidents$MAX_SEV_IR
Accidents <- Accidents %>%
  mutate_at(
    .vars = "INJURY",
    .funs = c(function(x) ifelse(x == "1" | x == "2", "Yes", "No"))
  )
Accidents$INJURY <- as.factor(Accidents$INJURY)

# Look at the summary of Injury
summary(Accidents$INJURY)
```

Using the Naive Rule, we would predict a new accident to have an injury involved since INJURY = "yes" is more common than "no".

## (b) Select the first 12 records in the dataset and look only at the response (INJURY) and
the two predictors WEATHER_R and TRAF_CON_R.

i. Using Excel tools create a pivot table that examines INJURY as a function of
the 2 predictors for these 12 records. Use all 3 variables in the pivot table as
rows/columns, and use counts for the cells.


``` {r}
# Select the required columns and the 12 records
df <- select(Accidents, c("WEATHER_R", "TRAF_CON_R", "INJURY"))[1:12, ]

# Create the pivot table
pivot_table <- as.data.frame(table(df$WEATHER_R, df$TRAF_CON_R, df$INJURY,
  dnn = c("WEATHER_R", "TRAF_CON_R", "INJURY")
))

print(pivot_table)
```

ii. Compute the exact Bayes conditional probabilities of an injury (INJURY =Yes)
given the six possible combinations of the predictors.

``` {r}
# Function to calculate probability manually
calculate_prob <- function(weather_r, traf_con_r) {
  filter(pivot_table, WEATHER_R == weather_r & TRAF_CON_R == traf_con_r & INJURY == "Yes")$Freq /
    sum(filter(pivot_table, WEATHER_R == weather_r & TRAF_CON_R == traf_con_r)$Freq)
}


cat("P(INJURY = Yes | WEATHER_R = 1, TRAF_CON_R = 0):", calculate_prob(1, 0), "\n")
cat("P(INJURY = Yes | WEATHER_R = 2, TRAF_CON_R = 0):", calculate_prob(2, 0), "\n")
cat("P(INJURY = Yes | WEATHER_R = 1, TRAF_CON_R = 1):", calculate_prob(1, 1), "\n")
cat("P(INJURY = Yes | WEATHER_R = 2, TRAF_CON_R = 1):", calculate_prob(2, 1), "\n")
cat("P(INJURY = Yes | WEATHER_R = 1, TRAF_CON_R = 2):", calculate_prob(1, 2), "\n")
cat("P(INJURY = Yes | WEATHER_R = 2, TRAF_CON_R = 2):", calculate_prob(2, 2), "\n")
```

iii. Classify the 12 accidents using these probabilities and a cutoff of 0.5.


Since the cutoff is 0.5, the only combination of attributes that has probability greater than 0.5 is when WEATHER_R = 1 and TRAF_CON_R = 0

``` {r}
# add a predictions column
df <- df %>%
  mutate(predictions = ifelse(WEATHER_R == 1 & TRAF_CON_R == 0, "Yes", "No"))

cat("The predictions are: \n")
print(df)
```

iv. Compute manually the naive Bayes conditional probability of an injury given
WEATHER_R = 1 and TRAF_CON_R = 1.

By looking at the pivot table, we can calculate P(INJURY = Yes | WEATHER_R = 1, TRAF_CON_R = 1)

``` {r}
# Manually calculate the probability
prob <- ((3 / 12) * ((2 / 3) * (0 / 3))) / (((3 / 12) * ((2 / 3) * (0 / 3))) + ((9 / 12) * ((3 / 9) * (2 / 9))))

cat("P(INJURY = Yes | WEATHER_R = 1, TRAF_CON_R = 1):", prob, "\n")
```


v. Run a naive Bayes classifier on the 12 records and 2 predictors. Obtain
probabilities and classifications for all 12 records. Compare this to the exact
Bayes classification. Are the resulting classifications equivalent? Is the
ranking (= ordering) of observations equivalent?

``` {r}
# Train the Naive Bayes Classifier
nb <- naiveBayes(INJURY ~ WEATHER_R + TRAF_CON_R, data = df[, 1:3])

pred.prob <- predict(nb, newdata = df[, 1:3], type = "raw")

pred.class <- data.frame(ifelse(pred.prob[, 1] - pred.prob[2] < 0, "Yes", "No"))
colnames(pred.class) <- "class"

actual_vs_predicted <- data.frame("actual" = df$INJURY, "predicted" = pred.class$class)
actual_vs_predicted$exact_bayes <- df$predictions
actual_vs_predicted$no_prob <- pred.prob[, 1]
actual_vs_predicted$yes_prob <- pred.prob[, 2]

cat("Actual vs Predicted Probabilities are: \n")
print(actual_vs_predicted)
```
The resulting classifications of the Naive Bayes Classifier and the Exact Bayes classifier are equivalent. Their ranking and ordering is also equivalent.

## (c) Let us now return to the entire dataset. Partition the data into training/validation sets.

i. Assuming that no information or initial reports about the accident itself are
available at the time of prediction (only location characteristics, weather
conditions, etc.), which predictors can we include in the analysis? (Use the 
Data_Codes sheet.)

If no information about the accident itself is available, we can include these predictor in the analysis - HOUR_I_R, ALIGN_R, WRK_ZONE, WKDY_I_R, INT_HWY, LIGHTCON_I_R, REL_RWY_R, SPD_LIM, SUR_CON, TRAF_CON_R,  WEATHER_R.

``` {r}
# Generate the training data indices
df <- Accidents[, c(1, 3, 5, 6, 7, 8, 11, 12, 14, 15, 16, 20, 25)]
indices <- sample(seq_len(nrow(df)), size = floor(0.6 * nrow(df)))

# Get training and validation data
train_data <- df[indices, ]
validation_data <- df[-indices, ]
```


ii. Run a naive Bayes classifier on the complete training set with the relevant
predictors (and INJURY as the response). Note that all predictors are
categorical. Show the classification matrix.

``` {r}
nb <- naiveBayes(INJURY ~ ., train_data)

pred.class <- predict(nb, newdata = train_data)

cat("The Classification Matrix is :\n")
confusionMatrix(data = pred.class, reference = train_data$INJURY)
```

iii. What is the overall error for the validation set?

``` {r}
pred.class <- predict(nb, newdata = validation_data)

cf <- confusionMatrix(data = pred.class, reference = validation_data$INJURY)

cat("The error rate is :", paste0(round(100 * (1 - cf$overall[[1]]), 4), "%"), "\n")
```

iv. What is the percent improvement relative to the naive rule (using the validation set)?

``` {r message = FALSE, warning = FALSE}
naive_cf <- confusionMatrix(
  data = as.factor(rep("Yes", dim(validation_data)[[1]])),
  reference = validation_data$INJURY
)

cat(
  "The difference between the Naive Bayes Classifier accuracy and the Naive Rule accuracy is:",
  paste0(round(100 * (cf$overall[[1]] - naive_cf$overall[[1]]), 4), "%"), "\n"
)
```

v. Examine the conditional probabilities output. Why do we get a probability of
zero for P(INJURY = No | SPD_LIM = 5)?

``` {r}
pivot_table <- as.data.frame(table(validation_data$INJURY, validation_data$SPD_LIM,
  dnn = c("INJURY", "SPD_LIM")
))


cat(
  "P(INJURY = No | SPD_LIM = 5):",
  filter(pivot_table, SPD_LIM == 5 & INJURY == "No")$Freq /
    sum(filter(pivot_table, SPD_LIM == 5)$Freq)
)


rm(list = ls())
```

The entire dataset of nearly 17000 records has only one case with INJURY = No and SPD_LIM = 5 so the probability is approximately 0.

---
